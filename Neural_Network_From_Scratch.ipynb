{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv(\"/content/sample_data/mnist_train_small.csv\", header = None)\n",
        "test = pd.read_csv(\"/content/sample_data/mnist_test.csv\", header = None)\n"
      ],
      "metadata": {
        "id": "EO2mt63lPMjz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_output, train_input = train.iloc[:, 0], train.iloc[:, 1:]\n",
        "test_output, test_input = test.iloc[:, 0], test.iloc[:, 1:]\n",
        "\n",
        "train_input = train_input.astype(np.float32) / 255.0\n",
        "train_output = train_output.astype(int)\n",
        "\n",
        "test_input = test_input.astype(np.float32) / 255.0\n",
        "test_output = test_output.astype(int)"
      ],
      "metadata": {
        "id": "ZbF7ShM9TEC0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input.iloc[:, 120:140]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Ab6F0atbRvoy",
        "outputId": "309812d4-59b9-46ef-c0c4-03f482ef934c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       121  122       123       124       125       126       127       128  \\\n",
              "0      0.0  0.0  0.094118  0.262745  0.262745  0.070588  0.000000  0.000000   \n",
              "1      0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "2      0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "3      0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "4      0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "...    ...  ...       ...       ...       ...       ...       ...       ...   \n",
              "19995  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "19996  0.0  0.0  0.000000  0.000000  0.000000  0.043137  0.517647  0.678431   \n",
              "19997  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "19998  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "19999  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "            129       130       131       132  133  134  135  136  137  138  \\\n",
              "0      0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1      0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2      0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "3      0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "4      0.000000  0.078431  0.807843  0.823529  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "...         ...       ...       ...       ...  ...  ...  ...  ...  ...  ...   \n",
              "19995  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "19996  0.039216  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "19997  0.164706  0.917647  0.996078  0.117647  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "19998  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "19999  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "       139  140  \n",
              "0      0.0  0.0  \n",
              "1      0.0  0.0  \n",
              "2      0.0  0.0  \n",
              "3      0.0  0.0  \n",
              "4      0.0  0.0  \n",
              "...    ...  ...  \n",
              "19995  0.0  0.0  \n",
              "19996  0.0  0.0  \n",
              "19997  0.0  0.0  \n",
              "19998  0.0  0.0  \n",
              "19999  0.0  0.0  \n",
              "\n",
              "[20000 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5145e214-ff44-4483-b64d-f7f18b521725\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.094118</td>\n",
              "      <td>0.262745</td>\n",
              "      <td>0.262745</td>\n",
              "      <td>0.070588</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078431</td>\n",
              "      <td>0.807843</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043137</td>\n",
              "      <td>0.517647</td>\n",
              "      <td>0.678431</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.164706</td>\n",
              "      <td>0.917647</td>\n",
              "      <td>0.996078</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 20 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5145e214-ff44-4483-b64d-f7f18b521725')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5145e214-ff44-4483-b64d-f7f18b521725 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5145e214-ff44-4483-b64d-f7f18b521725');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"train_input\",\n  \"rows\": 20000,\n  \"fields\": [\n    {\n      \"column\": 121,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11582255851837595,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 214,\n        \"samples\": [\n          0.11372549019607843,\n          0.21176470588235294,\n          0.0392156862745098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 122,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15471670906260745,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 242,\n        \"samples\": [\n          0.4470588235294118,\n          0.9882352941176471,\n          0.23137254901960785\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 123,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19425777195727062,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 249,\n        \"samples\": [\n          0.8823529411764706,\n          0.0196078431372549,\n          0.9098039215686274\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 124,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23548306140471734,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 255,\n        \"samples\": [\n          0.7647058823529411,\n          0.6470588235294118,\n          0.6666666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 125,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27059011092108076,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          0.7803921568627451,\n          0.9921568627450981,\n          0.6627450980392157\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 126,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3044063519762899,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          0.7215686274509804,\n          0.9882352941176471,\n          0.16862745098039217\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 127,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33053885479574135,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          0.11764705882352941,\n          0.5843137254901961,\n          0.0196078431372549\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 128,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33958272274992535,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          0.4588235294117647,\n          0.6196078431372549,\n          0.8196078431372549\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 129,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3355805200832803,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          0.4196078431372549,\n          0.8196078431372549,\n          0.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 130,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.31871496856391385,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          0.3411764705882353,\n          0.9411764705882353,\n          0.8470588235294118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 131,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2934153452523089,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          0.4549019607843137,\n          0.24313725490196078,\n          0.9254901960784314\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 132,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.25508283201673826,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          0.26666666666666666,\n          0.8705882352941177,\n          0.5411764705882353\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 133,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20467647853731918,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 253,\n        \"samples\": [\n          0.18823529411764706,\n          0.06274509803921569,\n          0.9137254901960784\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 134,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15380104015502175,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 247,\n        \"samples\": [\n          0.4392156862745098,\n          0.11764705882352941,\n          0.8980392156862745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 135,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11051654675035767,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 215,\n        \"samples\": [\n          0.4117647058823529,\n          0.8117647058823529,\n          0.29411764705882354\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 136,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07294778636457402,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 167,\n        \"samples\": [\n          0.13725490196078433,\n          0.3176470588235294,\n          0.8941176470588236\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 137,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.041793504080081346,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 96,\n        \"samples\": [\n          0.30980392156862746,\n          0.596078431372549,\n          0.21176470588235294\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 138,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01988484995935893,\n        \"min\": 0.0,\n        \"max\": 0.9921568627450981,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          0.9607843137254902,\n          0.41568627450980394,\n          0.792156862745098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 139,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012024623630134616,\n        \"min\": 0.0,\n        \"max\": 0.8666666666666667,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.23921568627450981,\n          0.03529411764705882,\n          0.6470588235294118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 140,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0016360509839216302,\n        \"min\": 0.0,\n        \"max\": 0.23137254901960785,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.23137254901960785,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "T7VRMid-TnU7",
        "outputId": "cadb91f6-ed87-499c-dd46-dd1c729d52d0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       7\n",
              "1       2\n",
              "2       1\n",
              "3       0\n",
              "4       4\n",
              "       ..\n",
              "9995    2\n",
              "9996    3\n",
              "9997    4\n",
              "9998    5\n",
              "9999    6\n",
              "Name: 0, Length: 10000, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hot_encode(output):\n",
        "  dic = {0: [1,0,0,0,0,0,0,0,0,0], 1: [0,1,0,0,0,0,0,0,0,0], 2: [0,0,1,0,0,0,0,0,0,0], 3: [0,0,0,1,0,0,0,0,0,0], 4: [0,0,0,0,1,0,0,0,0,0], 5: [0,0,0,0,0,1,0,0,0,0], 6: [0,0,0,0,0,0,1,0,0,0], 7: [0,0,0,0,0,0,0,1,0,0], 8: [0,0,0,0,0,0,0,0,1,0], 9: [0,0,0,0,0,0,0,0,0,1]}\n",
        "  output_matrix_hotencoded = np.zeros((len(output), 10))\n",
        "  for i in range(len(output)):\n",
        "    output_matrix_hotencoded[i] = dic[output[i]]\n",
        "  return output_matrix_hotencoded"
      ],
      "metadata": {
        "id": "L8tu0FKxUo7V"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1000\n",
        "epochs = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "weight_matrix_1 = np.random.randn(784,128) * np.sqrt(2/784) #(784 x 128)\n",
        "bias_matrix_1 = np.zeros((1, 128)) #(1 x 128)\n",
        "\n",
        "weight_matrix_2 = np.random.randn(128,10)  * np.sqrt(2/128) #(128 x 10)\n",
        "bias_matrix_2 = np.zeros((1, 10)) #(1 x 10)\n",
        "\n",
        "# train_input_matrix = train_input[index:index+batch_size]\n",
        "# train_output_matrix = train_output[index:index + batch_size]\n",
        "\n",
        "train_input, train_output = np.array(train_input), np.array(train_output)\n",
        "test_input, test_output = np.array(test_input), np.array(test_output)\n",
        "train_output_matrix_hotencoded = hot_encode(train_output)\n",
        "test_output_matrix_hotencoded = hot_encode(test_output)\n"
      ],
      "metadata": {
        "id": "Yx5PvoZMCWSU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(relu_input):\n",
        "  return np.maximum(0, relu_input)\n"
      ],
      "metadata": {
        "id": "NEhGvZj_az8A"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softMax(softMax_input):\n",
        "  return np.exp(softMax_input) / np.sum(np.exp(softMax_input), axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "q8GHEjc-c97p"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(softMax_output, train_output):\n",
        "  return -np.sum(train_output * np.log(softMax_output))"
      ],
      "metadata": {
        "id": "J56meuNfocH0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ssr(train_output_matrix_hotencoded, logits):\n",
        "    return 0.5 * np.mean((train_output_matrix_hotencoded - logits)**2)"
      ],
      "metadata": {
        "id": "ixy7IoSUSMBE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computes logits for a batch using a 2-layer network:\n",
        "\n",
        "z1 = XW1 + b1 → a1 = ReLU(z1) → logits = a1W2 + b2"
      ],
      "metadata": {
        "id": "txMU1IEjBdKp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcbb49bd"
      },
      "source": [
        "def forward_propagation(weight_matrix_1, bias_matrix_1, weight_matrix_2, bias_matrix_2, train_input_batch):\n",
        "\n",
        "  relu_input = train_input_batch @ weight_matrix_1 + bias_matrix_1 #(B x 128)\n",
        "  relu_output = ReLU(relu_input) #(B x 128)\n",
        "\n",
        "  logits = relu_output @ weight_matrix_2 + bias_matrix_2 #(B x 10)\n",
        "  # softMax_output = softMax(softMax_input) #(B x 10)\n",
        "\n",
        "  return relu_input, relu_output, logits"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward pass for a 2-layer fully connected network:\n",
        "\n",
        "Linear → ReLU → Linear.\n",
        "The output logits are compared against one-hot labels using a squared-error (SSR) loss."
      ],
      "metadata": {
        "id": "4RKBGg8iAQTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation for squared-error loss:\n",
        "# Gradients are computed via the chain rule w.r.t. W2, b2, then propagated through ReLU to W1, b1\n",
        "\n",
        "def back_propagation(weight_matrix_1, bias_matrix_1, weight_matrix_2, bias_matrix_2, relu_input, relu_output, train_input_batch, train_output_batch, logits, learning_rate):\n",
        "\n",
        "  batch_size = train_input_batch.shape[0]\n",
        "\n",
        "  dlogits = (logits - train_output_batch) / batch_size #(B x 10)\n",
        "\n",
        "  db2 = np.sum(dlogits, axis = 0, keepdims=True) #(1 x 10)\n",
        "  dW2 = (relu_output.T @ dlogits) #(128 x 10)\n",
        "  # Gradient through ReLU: zero out gradients where pre-activation was negative\n",
        "  db1 = np.sum(dlogits @ weight_matrix_2.T * (relu_input > 0), axis = 0, keepdims=True) #(1 x 128)\n",
        "  dW1 = (train_input_batch.T @ (dlogits @ weight_matrix_2.T * (relu_input > 0))) #(B x 128)\n",
        "\n",
        "  weight_matrix_1 -= learning_rate * dW1\n",
        "  bias_matrix_1 -= learning_rate * db1\n",
        "  weight_matrix_2 -= learning_rate * dW2\n",
        "  bias_matrix_2 -= learning_rate * db2\n",
        "\n",
        "  return weight_matrix_1, bias_matrix_1, weight_matrix_2, bias_matrix_2"
      ],
      "metadata": {
        "id": "lqEmJvE6DOZA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop with fixed learning rate"
      ],
      "metadata": {
        "id": "yQgUyQlNCApA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "  perm = np.random.permutation(len(train_input))\n",
        "  train_input = train_input[perm]\n",
        "  train_output_matrix_hotencoded = train_output_matrix_hotencoded[perm]\n",
        "\n",
        "  total_correct = 0\n",
        "  total_loss = 0.0\n",
        "\n",
        "  for index in range(0, len(train_input), batch_size):\n",
        "    train_input_batch = train_input[index:index+batch_size]\n",
        "    train_output_batch = train_output_matrix_hotencoded[index:index+batch_size]\n",
        "\n",
        "    relu_input, relu_output, logits = forward_propagation(weight_matrix_1, bias_matrix_1, weight_matrix_2, bias_matrix_2, train_input_batch)\n",
        "    new_weight_matrix_1, new_bias_matrix_1, new_weight_matrix_2, new_bias_matrix_2 = back_propagation(weight_matrix_1, bias_matrix_1, weight_matrix_2, bias_matrix_2, relu_input, relu_output, train_input_batch, train_output_batch, logits, learning_rate)\n",
        "\n",
        "    weight_matrix_1 = new_weight_matrix_1\n",
        "    bias_matrix_1 = new_bias_matrix_1\n",
        "    weight_matrix_2 = new_weight_matrix_2\n",
        "    bias_matrix_2 = new_bias_matrix_2\n",
        "\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    labels = np.argmax(train_output_batch, axis=1)\n",
        "    total_correct += np.sum(preds == labels)\n",
        "    total_loss += ssr(train_output_batch, logits) * len(train_input_batch)  # weight by batch size\n",
        "\n",
        "  print(f\"Epoch {epoch+1}: train_acc={total_correct/len(train_input):.4f}  train_loss={total_loss/len(train_input):.6f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH_B751ujbKH",
        "outputId": "9fa44ac3-d9c0-4a87-a448-77e4f5a0a56a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_acc=0.4102  train_loss=0.057657\n",
            "Epoch 2: train_acc=0.6664  train_loss=0.030514\n",
            "Epoch 3: train_acc=0.7449  train_loss=0.026407\n",
            "Epoch 4: train_acc=0.7883  train_loss=0.023760\n",
            "Epoch 5: train_acc=0.8177  train_loss=0.021779\n",
            "Epoch 6: train_acc=0.8379  train_loss=0.020196\n",
            "Epoch 7: train_acc=0.8534  train_loss=0.018902\n",
            "Epoch 8: train_acc=0.8642  train_loss=0.017839\n",
            "Epoch 9: train_acc=0.8739  train_loss=0.016941\n",
            "Epoch 10: train_acc=0.8812  train_loss=0.016176\n",
            "Epoch 11: train_acc=0.8853  train_loss=0.015522\n",
            "Epoch 12: train_acc=0.8898  train_loss=0.014947\n",
            "Epoch 13: train_acc=0.8930  train_loss=0.014445\n",
            "Epoch 14: train_acc=0.8968  train_loss=0.013998\n",
            "Epoch 15: train_acc=0.8998  train_loss=0.013601\n",
            "Epoch 16: train_acc=0.9020  train_loss=0.013248\n",
            "Epoch 17: train_acc=0.9049  train_loss=0.012928\n",
            "Epoch 18: train_acc=0.9054  train_loss=0.012642\n",
            "Epoch 19: train_acc=0.9082  train_loss=0.012379\n",
            "Epoch 20: train_acc=0.9094  train_loss=0.012143\n",
            "Epoch 21: train_acc=0.9119  train_loss=0.011926\n",
            "Epoch 22: train_acc=0.9137  train_loss=0.011721\n",
            "Epoch 23: train_acc=0.9153  train_loss=0.011534\n",
            "Epoch 24: train_acc=0.9164  train_loss=0.011357\n",
            "Epoch 25: train_acc=0.9168  train_loss=0.011195\n",
            "Epoch 26: train_acc=0.9183  train_loss=0.011041\n",
            "Epoch 27: train_acc=0.9192  train_loss=0.010896\n",
            "Epoch 28: train_acc=0.9201  train_loss=0.010758\n",
            "Epoch 29: train_acc=0.9215  train_loss=0.010628\n",
            "Epoch 30: train_acc=0.9225  train_loss=0.010507\n",
            "Epoch 31: train_acc=0.9235  train_loss=0.010393\n",
            "Epoch 32: train_acc=0.9241  train_loss=0.010282\n",
            "Epoch 33: train_acc=0.9247  train_loss=0.010179\n",
            "Epoch 34: train_acc=0.9255  train_loss=0.010077\n",
            "Epoch 35: train_acc=0.9262  train_loss=0.009981\n",
            "Epoch 36: train_acc=0.9277  train_loss=0.009889\n",
            "Epoch 37: train_acc=0.9276  train_loss=0.009803\n",
            "Epoch 38: train_acc=0.9288  train_loss=0.009717\n",
            "Epoch 39: train_acc=0.9294  train_loss=0.009636\n",
            "Epoch 40: train_acc=0.9300  train_loss=0.009555\n",
            "Epoch 41: train_acc=0.9301  train_loss=0.009479\n",
            "Epoch 42: train_acc=0.9315  train_loss=0.009409\n",
            "Epoch 43: train_acc=0.9316  train_loss=0.009335\n",
            "Epoch 44: train_acc=0.9325  train_loss=0.009266\n",
            "Epoch 45: train_acc=0.9332  train_loss=0.009198\n",
            "Epoch 46: train_acc=0.9335  train_loss=0.009134\n",
            "Epoch 47: train_acc=0.9344  train_loss=0.009073\n",
            "Epoch 48: train_acc=0.9346  train_loss=0.009013\n",
            "Epoch 49: train_acc=0.9346  train_loss=0.008954\n",
            "Epoch 50: train_acc=0.9355  train_loss=0.008896\n",
            "Epoch 51: train_acc=0.9361  train_loss=0.008843\n",
            "Epoch 52: train_acc=0.9367  train_loss=0.008785\n",
            "Epoch 53: train_acc=0.9368  train_loss=0.008733\n",
            "Epoch 54: train_acc=0.9375  train_loss=0.008682\n",
            "Epoch 55: train_acc=0.9380  train_loss=0.008633\n",
            "Epoch 56: train_acc=0.9383  train_loss=0.008583\n",
            "Epoch 57: train_acc=0.9386  train_loss=0.008537\n",
            "Epoch 58: train_acc=0.9389  train_loss=0.008491\n",
            "Epoch 59: train_acc=0.9394  train_loss=0.008444\n",
            "Epoch 60: train_acc=0.9394  train_loss=0.008401\n",
            "Epoch 61: train_acc=0.9403  train_loss=0.008354\n",
            "Epoch 62: train_acc=0.9398  train_loss=0.008313\n",
            "Epoch 63: train_acc=0.9409  train_loss=0.008271\n",
            "Epoch 64: train_acc=0.9408  train_loss=0.008230\n",
            "Epoch 65: train_acc=0.9415  train_loss=0.008190\n",
            "Epoch 66: train_acc=0.9415  train_loss=0.008150\n",
            "Epoch 67: train_acc=0.9415  train_loss=0.008114\n",
            "Epoch 68: train_acc=0.9418  train_loss=0.008076\n",
            "Epoch 69: train_acc=0.9419  train_loss=0.008039\n",
            "Epoch 70: train_acc=0.9427  train_loss=0.008002\n",
            "Epoch 71: train_acc=0.9432  train_loss=0.007965\n",
            "Epoch 72: train_acc=0.9435  train_loss=0.007931\n",
            "Epoch 73: train_acc=0.9432  train_loss=0.007896\n",
            "Epoch 74: train_acc=0.9437  train_loss=0.007862\n",
            "Epoch 75: train_acc=0.9444  train_loss=0.007827\n",
            "Epoch 76: train_acc=0.9446  train_loss=0.007795\n",
            "Epoch 77: train_acc=0.9448  train_loss=0.007764\n",
            "Epoch 78: train_acc=0.9452  train_loss=0.007730\n",
            "Epoch 79: train_acc=0.9457  train_loss=0.007698\n",
            "Epoch 80: train_acc=0.9461  train_loss=0.007670\n",
            "Epoch 81: train_acc=0.9460  train_loss=0.007640\n",
            "Epoch 82: train_acc=0.9465  train_loss=0.007608\n",
            "Epoch 83: train_acc=0.9475  train_loss=0.007580\n",
            "Epoch 84: train_acc=0.9473  train_loss=0.007549\n",
            "Epoch 85: train_acc=0.9479  train_loss=0.007520\n",
            "Epoch 86: train_acc=0.9482  train_loss=0.007493\n",
            "Epoch 87: train_acc=0.9483  train_loss=0.007466\n",
            "Epoch 88: train_acc=0.9487  train_loss=0.007439\n",
            "Epoch 89: train_acc=0.9487  train_loss=0.007413\n",
            "Epoch 90: train_acc=0.9488  train_loss=0.007387\n",
            "Epoch 91: train_acc=0.9496  train_loss=0.007358\n",
            "Epoch 92: train_acc=0.9501  train_loss=0.007335\n",
            "Epoch 93: train_acc=0.9501  train_loss=0.007307\n",
            "Epoch 94: train_acc=0.9504  train_loss=0.007283\n",
            "Epoch 95: train_acc=0.9506  train_loss=0.007257\n",
            "Epoch 96: train_acc=0.9507  train_loss=0.007233\n",
            "Epoch 97: train_acc=0.9508  train_loss=0.007212\n",
            "Epoch 98: train_acc=0.9517  train_loss=0.007184\n",
            "Epoch 99: train_acc=0.9517  train_loss=0.007162\n",
            "Epoch 100: train_acc=0.9513  train_loss=0.007138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_correct = 0\n",
        "\n",
        "batch = 1\n",
        "\n",
        "for index in range(0, len(test_input), batch_size):\n",
        "\n",
        "  total_loss = 0.0\n",
        "\n",
        "  test_input_batch = test_input[index:index+batch_size]\n",
        "  test_output_batch = test_output_matrix_hotencoded[index:index+batch_size]\n",
        "\n",
        "  relu_input, relu_output, logits = forward_propagation(\n",
        "      weight_matrix_1, bias_matrix_1, weight_matrix_2, bias_matrix_2,\n",
        "      test_input_batch,\n",
        "  )\n",
        "\n",
        "  preds = np.argmax(logits, axis=1)\n",
        "  labels = np.argmax(test_output_batch, axis=1)\n",
        "  total_correct += np.sum(preds == labels)\n",
        "  total_loss = ssr(test_output_batch, logits) * len(test_input_batch)\n",
        "\n",
        "  print(f\"Batch {batch}: test_loss={total_loss/len(test_input_batch):.6f}\")\n",
        "  batch += 1\n",
        "\n",
        "print(\"Test accuracy:\", total_correct / len(test_input))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhYDFaSyu9YW",
        "outputId": "217c21c6-946d-4305-fca0-6991072d3061"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: test_loss=0.008520\n",
            "Batch 2: test_loss=0.010187\n",
            "Batch 3: test_loss=0.009078\n",
            "Batch 4: test_loss=0.009256\n",
            "Batch 5: test_loss=0.009185\n",
            "Batch 6: test_loss=0.006082\n",
            "Batch 7: test_loss=0.006552\n",
            "Batch 8: test_loss=0.005509\n",
            "Batch 9: test_loss=0.004843\n",
            "Batch 10: test_loss=0.007732\n",
            "Test accuracy: 0.945\n"
          ]
        }
      ]
    }
  ]
}